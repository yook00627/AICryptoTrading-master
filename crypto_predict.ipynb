{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pair': 'BTC_ETH',\n",
       " 'period': 300,\n",
       " 'input_size': 256,\n",
       " 'output_size': 16,\n",
       " 'lstm_hidden_size': 64,\n",
       " 'columns': ['Close', 'Volume', 'Low', 'High'],\n",
       " 'csv_src_file': 'BTC_ETH',\n",
       " 'name': 'lstm',\n",
       " 'folder': {'data': 'data/', 'weights': 'weights/'},\n",
       " 'filename': 'BTC_ETH_lstm_i256_o16_Close_Volume_Low_High'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from config import CONFIG\n",
    "from utils import series_to_supervised\n",
    "\n",
    "CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Close</th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199523</th>\n",
       "      <td>0.113180</td>\n",
       "      <td>1498867500</td>\n",
       "      <td>0.113700</td>\n",
       "      <td>0.113145</td>\n",
       "      <td>0.113689</td>\n",
       "      <td>81.279295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199524</th>\n",
       "      <td>0.113170</td>\n",
       "      <td>1498867800</td>\n",
       "      <td>0.113222</td>\n",
       "      <td>0.113081</td>\n",
       "      <td>0.113222</td>\n",
       "      <td>48.004959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199525</th>\n",
       "      <td>0.112989</td>\n",
       "      <td>1498868100</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>0.112719</td>\n",
       "      <td>0.113100</td>\n",
       "      <td>86.614988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199526</th>\n",
       "      <td>0.114000</td>\n",
       "      <td>1498868400</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.112950</td>\n",
       "      <td>0.113000</td>\n",
       "      <td>132.173177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199527</th>\n",
       "      <td>0.114032</td>\n",
       "      <td>1498868700</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.113994</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>115.782015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Close   Timestamp      High       Low      Open      Volume\n",
       "199523  0.113180  1498867500  0.113700  0.113145  0.113689   81.279295\n",
       "199524  0.113170  1498867800  0.113222  0.113081  0.113222   48.004959\n",
       "199525  0.112989  1498868100  0.113100  0.112719  0.113100   86.614988\n",
       "199526  0.114000  1498868400  0.114208  0.112950  0.113000  132.173177\n",
       "199527  0.114032  1498868700  0.114208  0.113994  0.114000  115.782015"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from matplotlib import pyplot\n",
    " \n",
    "#data file path\n",
    "dfp = ''.join([CONFIG['folder']['data'], CONFIG['csv_src_file'], '.csv'])\n",
    "\n",
    "#Columns of price data to use\n",
    "columns = CONFIG['columns']\n",
    "# df = pd.read_csv(dfp).dropna().tail(1000000)\n",
    "dataset = pd.read_csv(dfp)\n",
    "\n",
    "# to drop values before 2018 1514764800, March 2018 1519862400, July 2017 1498867200\n",
    "dataset = dataset[dataset.Timestamp > 1498867200]\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = dataset.loc[:,columns].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 4, 16)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parameters to prepare the dataset for learning \n",
    "n_lag = CONFIG['input_size']\n",
    "n_out = CONFIG['output_size']\n",
    "n_features = len(columns)\n",
    "n_lag,n_features,n_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled = scaler.fit_transform(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-256)</th>\n",
       "      <th>var2(t-256)</th>\n",
       "      <th>var3(t-256)</th>\n",
       "      <th>var4(t-256)</th>\n",
       "      <th>var1(t-255)</th>\n",
       "      <th>var2(t-255)</th>\n",
       "      <th>var3(t-255)</th>\n",
       "      <th>var4(t-255)</th>\n",
       "      <th>var1(t-254)</th>\n",
       "      <th>var2(t-254)</th>\n",
       "      <th>...</th>\n",
       "      <th>var3(t+13)</th>\n",
       "      <th>var4(t+13)</th>\n",
       "      <th>var1(t+14)</th>\n",
       "      <th>var2(t+14)</th>\n",
       "      <th>var3(t+14)</th>\n",
       "      <th>var4(t+14)</th>\n",
       "      <th>var1(t+15)</th>\n",
       "      <th>var2(t+15)</th>\n",
       "      <th>var3(t+15)</th>\n",
       "      <th>var4(t+15)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.905547</td>\n",
       "      <td>0.023283</td>\n",
       "      <td>0.912411</td>\n",
       "      <td>0.907633</td>\n",
       "      <td>0.905449</td>\n",
       "      <td>0.013751</td>\n",
       "      <td>0.911762</td>\n",
       "      <td>0.902790</td>\n",
       "      <td>0.903609</td>\n",
       "      <td>0.024811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.852787</td>\n",
       "      <td>0.845378</td>\n",
       "      <td>0.843947</td>\n",
       "      <td>0.018096</td>\n",
       "      <td>0.849640</td>\n",
       "      <td>0.844363</td>\n",
       "      <td>0.843416</td>\n",
       "      <td>0.031796</td>\n",
       "      <td>0.849727</td>\n",
       "      <td>0.845093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0.905449</td>\n",
       "      <td>0.013751</td>\n",
       "      <td>0.911762</td>\n",
       "      <td>0.902790</td>\n",
       "      <td>0.903609</td>\n",
       "      <td>0.024811</td>\n",
       "      <td>0.908069</td>\n",
       "      <td>0.901559</td>\n",
       "      <td>0.913858</td>\n",
       "      <td>0.037862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849640</td>\n",
       "      <td>0.844363</td>\n",
       "      <td>0.843416</td>\n",
       "      <td>0.031796</td>\n",
       "      <td>0.849727</td>\n",
       "      <td>0.845093</td>\n",
       "      <td>0.846576</td>\n",
       "      <td>0.018566</td>\n",
       "      <td>0.848707</td>\n",
       "      <td>0.844701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>0.903609</td>\n",
       "      <td>0.024811</td>\n",
       "      <td>0.908069</td>\n",
       "      <td>0.901559</td>\n",
       "      <td>0.913858</td>\n",
       "      <td>0.037862</td>\n",
       "      <td>0.910422</td>\n",
       "      <td>0.912771</td>\n",
       "      <td>0.914182</td>\n",
       "      <td>0.033166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.849727</td>\n",
       "      <td>0.845093</td>\n",
       "      <td>0.846576</td>\n",
       "      <td>0.018566</td>\n",
       "      <td>0.848707</td>\n",
       "      <td>0.844701</td>\n",
       "      <td>0.853820</td>\n",
       "      <td>0.051177</td>\n",
       "      <td>0.853399</td>\n",
       "      <td>0.854308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.913858</td>\n",
       "      <td>0.037862</td>\n",
       "      <td>0.910422</td>\n",
       "      <td>0.912771</td>\n",
       "      <td>0.914182</td>\n",
       "      <td>0.033166</td>\n",
       "      <td>0.921074</td>\n",
       "      <td>0.912771</td>\n",
       "      <td>0.908629</td>\n",
       "      <td>0.032194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.848707</td>\n",
       "      <td>0.844701</td>\n",
       "      <td>0.853820</td>\n",
       "      <td>0.051177</td>\n",
       "      <td>0.853399</td>\n",
       "      <td>0.854308</td>\n",
       "      <td>0.860146</td>\n",
       "      <td>0.035397</td>\n",
       "      <td>0.859355</td>\n",
       "      <td>0.860054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.914182</td>\n",
       "      <td>0.033166</td>\n",
       "      <td>0.921074</td>\n",
       "      <td>0.912771</td>\n",
       "      <td>0.908629</td>\n",
       "      <td>0.032194</td>\n",
       "      <td>0.914275</td>\n",
       "      <td>0.912694</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.022547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.853399</td>\n",
       "      <td>0.854308</td>\n",
       "      <td>0.860146</td>\n",
       "      <td>0.035397</td>\n",
       "      <td>0.859355</td>\n",
       "      <td>0.860054</td>\n",
       "      <td>0.867555</td>\n",
       "      <td>0.063028</td>\n",
       "      <td>0.867069</td>\n",
       "      <td>0.865115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.908629</td>\n",
       "      <td>0.032194</td>\n",
       "      <td>0.914275</td>\n",
       "      <td>0.912694</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.022547</td>\n",
       "      <td>0.910822</td>\n",
       "      <td>0.909222</td>\n",
       "      <td>0.899960</td>\n",
       "      <td>0.035727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859355</td>\n",
       "      <td>0.860054</td>\n",
       "      <td>0.867555</td>\n",
       "      <td>0.063028</td>\n",
       "      <td>0.867069</td>\n",
       "      <td>0.865115</td>\n",
       "      <td>0.865768</td>\n",
       "      <td>0.063221</td>\n",
       "      <td>0.872728</td>\n",
       "      <td>0.865894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.022547</td>\n",
       "      <td>0.910822</td>\n",
       "      <td>0.909222</td>\n",
       "      <td>0.899960</td>\n",
       "      <td>0.035727</td>\n",
       "      <td>0.905876</td>\n",
       "      <td>0.900546</td>\n",
       "      <td>0.897423</td>\n",
       "      <td>0.061659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.867069</td>\n",
       "      <td>0.865115</td>\n",
       "      <td>0.865768</td>\n",
       "      <td>0.063221</td>\n",
       "      <td>0.872728</td>\n",
       "      <td>0.865894</td>\n",
       "      <td>0.855803</td>\n",
       "      <td>0.031214</td>\n",
       "      <td>0.862697</td>\n",
       "      <td>0.863259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>0.899960</td>\n",
       "      <td>0.035727</td>\n",
       "      <td>0.905876</td>\n",
       "      <td>0.900546</td>\n",
       "      <td>0.897423</td>\n",
       "      <td>0.061659</td>\n",
       "      <td>0.904575</td>\n",
       "      <td>0.897803</td>\n",
       "      <td>0.904223</td>\n",
       "      <td>0.029123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.872728</td>\n",
       "      <td>0.865894</td>\n",
       "      <td>0.855803</td>\n",
       "      <td>0.031214</td>\n",
       "      <td>0.862697</td>\n",
       "      <td>0.863259</td>\n",
       "      <td>0.856094</td>\n",
       "      <td>0.011984</td>\n",
       "      <td>0.862677</td>\n",
       "      <td>0.853870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>0.897423</td>\n",
       "      <td>0.061659</td>\n",
       "      <td>0.904575</td>\n",
       "      <td>0.897803</td>\n",
       "      <td>0.904223</td>\n",
       "      <td>0.029123</td>\n",
       "      <td>0.904403</td>\n",
       "      <td>0.901062</td>\n",
       "      <td>0.900683</td>\n",
       "      <td>0.037249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862697</td>\n",
       "      <td>0.863259</td>\n",
       "      <td>0.856094</td>\n",
       "      <td>0.011984</td>\n",
       "      <td>0.862677</td>\n",
       "      <td>0.853870</td>\n",
       "      <td>0.854065</td>\n",
       "      <td>0.018126</td>\n",
       "      <td>0.860696</td>\n",
       "      <td>0.853351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>0.904223</td>\n",
       "      <td>0.029123</td>\n",
       "      <td>0.904403</td>\n",
       "      <td>0.901062</td>\n",
       "      <td>0.900683</td>\n",
       "      <td>0.037249</td>\n",
       "      <td>0.905831</td>\n",
       "      <td>0.902202</td>\n",
       "      <td>0.905750</td>\n",
       "      <td>0.011936</td>\n",
       "      <td>...</td>\n",
       "      <td>0.862677</td>\n",
       "      <td>0.853870</td>\n",
       "      <td>0.854065</td>\n",
       "      <td>0.018126</td>\n",
       "      <td>0.860696</td>\n",
       "      <td>0.853351</td>\n",
       "      <td>0.857997</td>\n",
       "      <td>0.041955</td>\n",
       "      <td>0.859928</td>\n",
       "      <td>0.854992</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1088 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     var1(t-256)  var2(t-256)  var3(t-256)  var4(t-256)  var1(t-255)  \\\n",
       "256     0.905547     0.023283     0.912411     0.907633     0.905449   \n",
       "257     0.905449     0.013751     0.911762     0.902790     0.903609   \n",
       "258     0.903609     0.024811     0.908069     0.901559     0.913858   \n",
       "259     0.913858     0.037862     0.910422     0.912771     0.914182   \n",
       "260     0.914182     0.033166     0.921074     0.912771     0.908629   \n",
       "261     0.908629     0.032194     0.914275     0.912694     0.903614   \n",
       "262     0.903614     0.022547     0.910822     0.909222     0.899960   \n",
       "263     0.899960     0.035727     0.905876     0.900546     0.897423   \n",
       "264     0.897423     0.061659     0.904575     0.897803     0.904223   \n",
       "265     0.904223     0.029123     0.904403     0.901062     0.900683   \n",
       "\n",
       "     var2(t-255)  var3(t-255)  var4(t-255)  var1(t-254)  var2(t-254)  \\\n",
       "256     0.013751     0.911762     0.902790     0.903609     0.024811   \n",
       "257     0.024811     0.908069     0.901559     0.913858     0.037862   \n",
       "258     0.037862     0.910422     0.912771     0.914182     0.033166   \n",
       "259     0.033166     0.921074     0.912771     0.908629     0.032194   \n",
       "260     0.032194     0.914275     0.912694     0.903614     0.022547   \n",
       "261     0.022547     0.910822     0.909222     0.899960     0.035727   \n",
       "262     0.035727     0.905876     0.900546     0.897423     0.061659   \n",
       "263     0.061659     0.904575     0.897803     0.904223     0.029123   \n",
       "264     0.029123     0.904403     0.901062     0.900683     0.037249   \n",
       "265     0.037249     0.905831     0.902202     0.905750     0.011936   \n",
       "\n",
       "        ...      var3(t+13)  var4(t+13)  var1(t+14)  var2(t+14)  var3(t+14)  \\\n",
       "256     ...        0.852787    0.845378    0.843947    0.018096    0.849640   \n",
       "257     ...        0.849640    0.844363    0.843416    0.031796    0.849727   \n",
       "258     ...        0.849727    0.845093    0.846576    0.018566    0.848707   \n",
       "259     ...        0.848707    0.844701    0.853820    0.051177    0.853399   \n",
       "260     ...        0.853399    0.854308    0.860146    0.035397    0.859355   \n",
       "261     ...        0.859355    0.860054    0.867555    0.063028    0.867069   \n",
       "262     ...        0.867069    0.865115    0.865768    0.063221    0.872728   \n",
       "263     ...        0.872728    0.865894    0.855803    0.031214    0.862697   \n",
       "264     ...        0.862697    0.863259    0.856094    0.011984    0.862677   \n",
       "265     ...        0.862677    0.853870    0.854065    0.018126    0.860696   \n",
       "\n",
       "     var4(t+14)  var1(t+15)  var2(t+15)  var3(t+15)  var4(t+15)  \n",
       "256    0.844363    0.843416    0.031796    0.849727    0.845093  \n",
       "257    0.845093    0.846576    0.018566    0.848707    0.844701  \n",
       "258    0.844701    0.853820    0.051177    0.853399    0.854308  \n",
       "259    0.854308    0.860146    0.035397    0.859355    0.860054  \n",
       "260    0.860054    0.867555    0.063028    0.867069    0.865115  \n",
       "261    0.865115    0.865768    0.063221    0.872728    0.865894  \n",
       "262    0.865894    0.855803    0.031214    0.862697    0.863259  \n",
       "263    0.863259    0.856094    0.011984    0.862677    0.853870  \n",
       "264    0.853870    0.854065    0.018126    0.860696    0.853351  \n",
       "265    0.853351    0.857997    0.041955    0.859928    0.854992  \n",
       "\n",
       "[10 rows x 1088 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# frame as supervised learning\n",
    "reframed = series_to_supervised(scaled, n_lag, n_out)\n",
    "reframed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var1(t-256)</th>\n",
       "      <th>var2(t-256)</th>\n",
       "      <th>var3(t-256)</th>\n",
       "      <th>var4(t-256)</th>\n",
       "      <th>var1(t-255)</th>\n",
       "      <th>var2(t-255)</th>\n",
       "      <th>var3(t-255)</th>\n",
       "      <th>var4(t-255)</th>\n",
       "      <th>var1(t-254)</th>\n",
       "      <th>var2(t-254)</th>\n",
       "      <th>...</th>\n",
       "      <th>var1(t+6)</th>\n",
       "      <th>var1(t+7)</th>\n",
       "      <th>var1(t+8)</th>\n",
       "      <th>var1(t+9)</th>\n",
       "      <th>var1(t+10)</th>\n",
       "      <th>var1(t+11)</th>\n",
       "      <th>var1(t+12)</th>\n",
       "      <th>var1(t+13)</th>\n",
       "      <th>var1(t+14)</th>\n",
       "      <th>var1(t+15)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>0.905547</td>\n",
       "      <td>0.023283</td>\n",
       "      <td>0.912411</td>\n",
       "      <td>0.907633</td>\n",
       "      <td>0.905449</td>\n",
       "      <td>0.013751</td>\n",
       "      <td>0.911762</td>\n",
       "      <td>0.902790</td>\n",
       "      <td>0.903609</td>\n",
       "      <td>0.024811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.839917</td>\n",
       "      <td>0.833256</td>\n",
       "      <td>0.835041</td>\n",
       "      <td>0.838779</td>\n",
       "      <td>0.838864</td>\n",
       "      <td>0.839877</td>\n",
       "      <td>0.845957</td>\n",
       "      <td>0.847477</td>\n",
       "      <td>0.843947</td>\n",
       "      <td>0.843416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>0.905449</td>\n",
       "      <td>0.013751</td>\n",
       "      <td>0.911762</td>\n",
       "      <td>0.902790</td>\n",
       "      <td>0.903609</td>\n",
       "      <td>0.024811</td>\n",
       "      <td>0.908069</td>\n",
       "      <td>0.901559</td>\n",
       "      <td>0.913858</td>\n",
       "      <td>0.037862</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833256</td>\n",
       "      <td>0.835041</td>\n",
       "      <td>0.838779</td>\n",
       "      <td>0.838864</td>\n",
       "      <td>0.839877</td>\n",
       "      <td>0.845957</td>\n",
       "      <td>0.847477</td>\n",
       "      <td>0.843947</td>\n",
       "      <td>0.843416</td>\n",
       "      <td>0.846576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>0.903609</td>\n",
       "      <td>0.024811</td>\n",
       "      <td>0.908069</td>\n",
       "      <td>0.901559</td>\n",
       "      <td>0.913858</td>\n",
       "      <td>0.037862</td>\n",
       "      <td>0.910422</td>\n",
       "      <td>0.912771</td>\n",
       "      <td>0.914182</td>\n",
       "      <td>0.033166</td>\n",
       "      <td>...</td>\n",
       "      <td>0.835041</td>\n",
       "      <td>0.838779</td>\n",
       "      <td>0.838864</td>\n",
       "      <td>0.839877</td>\n",
       "      <td>0.845957</td>\n",
       "      <td>0.847477</td>\n",
       "      <td>0.843947</td>\n",
       "      <td>0.843416</td>\n",
       "      <td>0.846576</td>\n",
       "      <td>0.853820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.913858</td>\n",
       "      <td>0.037862</td>\n",
       "      <td>0.910422</td>\n",
       "      <td>0.912771</td>\n",
       "      <td>0.914182</td>\n",
       "      <td>0.033166</td>\n",
       "      <td>0.921074</td>\n",
       "      <td>0.912771</td>\n",
       "      <td>0.908629</td>\n",
       "      <td>0.032194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838779</td>\n",
       "      <td>0.838864</td>\n",
       "      <td>0.839877</td>\n",
       "      <td>0.845957</td>\n",
       "      <td>0.847477</td>\n",
       "      <td>0.843947</td>\n",
       "      <td>0.843416</td>\n",
       "      <td>0.846576</td>\n",
       "      <td>0.853820</td>\n",
       "      <td>0.860146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>0.914182</td>\n",
       "      <td>0.033166</td>\n",
       "      <td>0.921074</td>\n",
       "      <td>0.912771</td>\n",
       "      <td>0.908629</td>\n",
       "      <td>0.032194</td>\n",
       "      <td>0.914275</td>\n",
       "      <td>0.912694</td>\n",
       "      <td>0.903614</td>\n",
       "      <td>0.022547</td>\n",
       "      <td>...</td>\n",
       "      <td>0.838864</td>\n",
       "      <td>0.839877</td>\n",
       "      <td>0.845957</td>\n",
       "      <td>0.847477</td>\n",
       "      <td>0.843947</td>\n",
       "      <td>0.843416</td>\n",
       "      <td>0.846576</td>\n",
       "      <td>0.853820</td>\n",
       "      <td>0.860146</td>\n",
       "      <td>0.867555</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1040 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     var1(t-256)  var2(t-256)  var3(t-256)  var4(t-256)  var1(t-255)  \\\n",
       "256     0.905547     0.023283     0.912411     0.907633     0.905449   \n",
       "257     0.905449     0.013751     0.911762     0.902790     0.903609   \n",
       "258     0.903609     0.024811     0.908069     0.901559     0.913858   \n",
       "259     0.913858     0.037862     0.910422     0.912771     0.914182   \n",
       "260     0.914182     0.033166     0.921074     0.912771     0.908629   \n",
       "\n",
       "     var2(t-255)  var3(t-255)  var4(t-255)  var1(t-254)  var2(t-254)  \\\n",
       "256     0.013751     0.911762     0.902790     0.903609     0.024811   \n",
       "257     0.024811     0.908069     0.901559     0.913858     0.037862   \n",
       "258     0.037862     0.910422     0.912771     0.914182     0.033166   \n",
       "259     0.033166     0.921074     0.912771     0.908629     0.032194   \n",
       "260     0.032194     0.914275     0.912694     0.903614     0.022547   \n",
       "\n",
       "        ...      var1(t+6)  var1(t+7)  var1(t+8)  var1(t+9)  var1(t+10)  \\\n",
       "256     ...       0.839917   0.833256   0.835041   0.838779    0.838864   \n",
       "257     ...       0.833256   0.835041   0.838779   0.838864    0.839877   \n",
       "258     ...       0.835041   0.838779   0.838864   0.839877    0.845957   \n",
       "259     ...       0.838779   0.838864   0.839877   0.845957    0.847477   \n",
       "260     ...       0.838864   0.839877   0.845957   0.847477    0.843947   \n",
       "\n",
       "     var1(t+11)  var1(t+12)  var1(t+13)  var1(t+14)  var1(t+15)  \n",
       "256    0.839877    0.845957    0.847477    0.843947    0.843416  \n",
       "257    0.845957    0.847477    0.843947    0.843416    0.846576  \n",
       "258    0.847477    0.843947    0.843416    0.846576    0.853820  \n",
       "259    0.843947    0.843416    0.846576    0.853820    0.860146  \n",
       "260    0.843416    0.846576    0.853820    0.860146    0.867555  \n",
       "\n",
       "[5 rows x 1040 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop columns we don't want to predict\n",
    "# We're only concerned with the estimating the close value,\n",
    "# Close should be first in the list of column in the config file\n",
    "\n",
    "cols_to_drop = []\n",
    "\n",
    "for i in range (n_out):\n",
    "    for j in range(1, n_features):\n",
    "        cols_to_drop.append(reframed.shape[1]-(i*n_features+j))\n",
    "\n",
    "reframed.drop(reframed.columns[cols_to_drop], axis=1, inplace=True)\n",
    "\n",
    "reframed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reframed_values = reframed.values\n",
    "# split into train and test sets\n",
    "training_size = int(0.8* reframed_values.shape[0])\n",
    "train = reframed_values[:training_size, :]\n",
    "test = reframed_values[training_size:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(79436, 256, 4) (79436, 16, 1) (19859, 256, 4) (19859, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "# split into input and outputs\n",
    "n_obs = n_lag * n_features\n",
    "\n",
    "# We're only concerned with the estimating the close value,\n",
    "# Close should be first in the list of column in the config file\n",
    "\n",
    "n_outputs = n_out * n_features\n",
    "train_x, train_y = train[:, :n_obs], train[:, -n_out:]\n",
    "test_x, test_y = test[:, :n_obs], test[:, -n_out:]\n",
    "\n",
    "# reshape input to be 3D [samples, timesteps, features]\n",
    "train_x = train_x.reshape((train_x.shape[0], n_lag, n_features))\n",
    "test_x = test_x.reshape((test_x.shape[0], n_lag, n_features))\n",
    "\n",
    "# reshape output to be 3D [samples, timesteps, features]\n",
    "train_y = train_y.reshape(-1, n_out, 1)\n",
    "test_y = test_y.reshape(-1, n_out, 1)\n",
    "\n",
    "print(train_x.shape, train_y.shape, test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=''.join([CONFIG['folder']['weights'], CONFIG['filename'], '_model', '.json'])\n",
    "model_weights_name=''.join([CONFIG['folder']['weights'], CONFIG['filename'], '_model_weights', '.h5'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yook00627\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 128)               35328     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 16, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 16, 32)            20608     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 32)            0         \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 16, 1)             33        \n",
      "=================================================================\n",
      "Total params: 55,969\n",
      "Trainable params: 55,969\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM, CuDNNLSTM, GRU,CuDNNGRU\n",
    "from keras.layers import Conv1D, AveragePooling1D, MaxPooling1D\n",
    "from keras.layers import Dropout, Flatten\n",
    "from keras.layers import Activation, BatchNormalization\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import RepeatVector\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "units= CONFIG['lstm_hidden_size']\n",
    "dropout = .25\n",
    "\n",
    "# design network\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Bidirectional(LSTM(units), input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(RepeatVector(n_out))\n",
    "\n",
    "model.add(LSTM(units//2, return_sequences=True))\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "# We're only concerned with the estimating the close value,\n",
    "# otherwise use n_outputs instead of 1\n",
    "# Dense(n_outputs, ...\n",
    "model.add(TimeDistributed(Dense(1, activation='tanh')))\n",
    "\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# store model\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(model_name, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=50\n",
    "batch_size=2560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 79436 samples, validate on 19859 samples\n",
      "Epoch 1/50\n",
      "79436/79436 [==============================] - 34s 425us/step - loss: 0.0478 - val_loss: 0.0108\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.01082, saving model to weights/BTC_ETH_lstm_i256_o16_Close_Volume_Low_High_model_weights.h5\n",
      "Epoch 2/50\n",
      "79436/79436 [==============================] - 31s 390us/step - loss: 0.0131 - val_loss: 0.0053\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.01082 to 0.00526, saving model to weights/BTC_ETH_lstm_i256_o16_Close_Volume_Low_High_model_weights.h5\n",
      "Epoch 3/50\n",
      "79436/79436 [==============================] - 31s 388us/step - loss: 0.0074 - val_loss: 0.0020\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.00526 to 0.00201, saving model to weights/BTC_ETH_lstm_i256_o16_Close_Volume_Low_High_model_weights.h5\n",
      "Epoch 4/50\n",
      "79436/79436 [==============================] - 31s 392us/step - loss: 0.0049 - val_loss: 0.0011\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.00201 to 0.00107, saving model to weights/BTC_ETH_lstm_i256_o16_Close_Volume_Low_High_model_weights.h5\n",
      "Epoch 5/50\n",
      "79436/79436 [==============================] - 31s 391us/step - loss: 0.0038 - val_loss: 5.4120e-04\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.00107 to 0.00054, saving model to weights/BTC_ETH_lstm_i256_o16_Close_Volume_Low_High_model_weights.h5\n",
      "Epoch 6/50\n",
      "79436/79436 [==============================] - 31s 389us/step - loss: 0.0031 - val_loss: 2.8824e-04\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.00054 to 0.00029, saving model to weights/BTC_ETH_lstm_i256_o16_Close_Volume_Low_High_model_weights.h5\n",
      "Epoch 7/50\n",
      "79436/79436 [==============================] - 32s 401us/step - loss: 0.0027 - val_loss: 1.7883e-04\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.00029 to 0.00018, saving model to weights/BTC_ETH_lstm_i256_o16_Close_Volume_Low_High_model_weights.h5\n",
      "Epoch 8/50\n",
      "79436/79436 [==============================] - 31s 392us/step - loss: 0.0024 - val_loss: 1.3541e-04\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.00018 to 0.00014, saving model to weights/BTC_ETH_lstm_i256_o16_Close_Volume_Low_High_model_weights.h5\n",
      "Epoch 9/50\n",
      "79436/79436 [==============================] - 31s 390us/step - loss: 0.0023 - val_loss: 1.3078e-04\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.00014 to 0.00013, saving model to weights/BTC_ETH_lstm_i256_o16_Close_Volume_Low_High_model_weights.h5\n",
      "Epoch 10/50\n",
      "79436/79436 [==============================] - 31s 389us/step - loss: 0.0021 - val_loss: 9.5012e-05\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.00013 to 0.00010, saving model to weights/BTC_ETH_lstm_i256_o16_Close_Volume_Low_High_model_weights.h5\n",
      "Epoch 11/50\n",
      "79436/79436 [==============================] - 31s 389us/step - loss: 0.0020 - val_loss: 9.4140e-05\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.00010 to 0.00009, saving model to weights/BTC_ETH_lstm_i256_o16_Close_Volume_Low_High_model_weights.h5\n",
      "Epoch 12/50\n",
      "79436/79436 [==============================] - 31s 389us/step - loss: 0.0019 - val_loss: 7.4969e-05\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.00009 to 0.00007, saving model to weights/BTC_ETH_lstm_i256_o16_Close_Volume_Low_High_model_weights.h5\n",
      "Epoch 13/50\n",
      "79436/79436 [==============================] - 32s 397us/step - loss: 0.0019 - val_loss: 6.8191e-05\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.00007 to 0.00007, saving model to weights/BTC_ETH_lstm_i256_o16_Close_Volume_Low_High_model_weights.h5\n",
      "Epoch 14/50\n",
      "79436/79436 [==============================] - 32s 399us/step - loss: 0.0018 - val_loss: 1.8376e-04\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.00007\n",
      "Epoch 15/50\n",
      "79436/79436 [==============================] - 31s 391us/step - loss: 0.0018 - val_loss: 6.1361e-05\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.00007 to 0.00006, saving model to weights/BTC_ETH_lstm_i256_o16_Close_Volume_Low_High_model_weights.h5\n",
      "Epoch 16/50\n",
      "79436/79436 [==============================] - 31s 392us/step - loss: 0.0017 - val_loss: 5.6706e-05\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.00006 to 0.00006, saving model to weights/BTC_ETH_lstm_i256_o16_Close_Volume_Low_High_model_weights.h5\n",
      "Epoch 17/50\n",
      "79436/79436 [==============================] - 31s 390us/step - loss: 0.0017 - val_loss: 7.1261e-05\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00006\n",
      "Epoch 18/50\n",
      "79436/79436 [==============================] - 31s 389us/step - loss: 0.0016 - val_loss: 6.4145e-05\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00006\n",
      "Epoch 19/50\n",
      "79436/79436 [==============================] - 31s 389us/step - loss: 0.0016 - val_loss: 5.7507e-05\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.00006\n",
      "Epoch 20/50\n",
      "79436/79436 [==============================] - 31s 389us/step - loss: 0.0016 - val_loss: 5.9596e-05\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00006\n",
      "Epoch 21/50\n",
      "79436/79436 [==============================] - 31s 390us/step - loss: 0.0016 - val_loss: 5.0063e-05\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.00006 to 0.00005, saving model to weights/BTC_ETH_lstm_i256_o16_Close_Volume_Low_High_model_weights.h5\n",
      "Epoch 22/50\n",
      "79436/79436 [==============================] - 31s 391us/step - loss: 0.0015 - val_loss: 2.0541e-04\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00005\n",
      "Epoch 23/50\n",
      "79436/79436 [==============================] - 31s 391us/step - loss: 0.0015 - val_loss: 5.3682e-05\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00005\n",
      "Epoch 24/50\n",
      "79436/79436 [==============================] - 31s 390us/step - loss: 0.0015 - val_loss: 5.5683e-05\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00005\n",
      "Epoch 25/50\n",
      "79436/79436 [==============================] - 31s 392us/step - loss: 0.0015 - val_loss: 6.1896e-05\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00005\n",
      "Epoch 26/50\n",
      "79436/79436 [==============================] - 31s 396us/step - loss: 0.0014 - val_loss: 1.2045e-04\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00005\n",
      "Epoch 27/50\n",
      "79436/79436 [==============================] - 31s 389us/step - loss: 0.0014 - val_loss: 5.9890e-05\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00005\n",
      "Epoch 28/50\n",
      "53760/79436 [===================>..........] - ETA: 8s - loss: 0.0014"
     ]
    }
   ],
   "source": [
    "# fit network\n",
    "history = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size,\n",
    "                    validation_data=(test_x, test_y), verbose=1, shuffle=True,\n",
    "                    callbacks=[ModelCheckpoint(model_weights_name, monitor='val_loss', verbose=1,save_best_only='true',\n",
    "                                              save_weights_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best weights\n",
    "model.load_weights(model_weights_name)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot history\n",
    "pyplot.plot(history.history['loss'], label='train')\n",
    "pyplot.plot(history.history['val_loss'], label='test')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the prediction of test data\n",
    "y = model.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = test_y[:,0]\n",
    "b = y[:,0]\n",
    "c = np.append(b, y[-1], axis=0)\n",
    "\n",
    "# Show how the model fits the test data\n",
    "pyplot.plot(a[:100], label='original')\n",
    "pyplot.plot(b[:100], label='model')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "# Show how the model predicts data\n",
    "pos = int(a.shape[0]-n_out*4)\n",
    "pyplot.plot(a[pos:], label='original')\n",
    "pyplot.plot(c[pos:], label='model')\n",
    "pyplot.legend()\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction on public data!\n",
    "period = CONFIG['period']\n",
    "import time\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "\n",
    "# Download a live bitcoin price data set\n",
    "def dl_X(now = None, points = n_lag, period = period, pair=CONFIG['pair']):\n",
    "    if now == None:\n",
    "        now = time.time() \n",
    "    end = now - now % period\n",
    "    #print end, time.strftime(\"%a, %d %b %Y %H:%M:%S +0000\", time.gmtime(end))\n",
    "    start = end - points*period\n",
    "    #print start, time.strftime(\"%a, %d %b %Y %H:%M:%S +0000\", time.gmtime(start))\n",
    "    url = \"https://poloniex.com/public?command=returnChartData&currencyPair=%s&start=%d&end=%d&period=%d\" % (pair, start, end, period)\n",
    "    openUrl = urlopen(url)\n",
    "    r = openUrl.read()\n",
    "    openUrl.close()\n",
    "    d = json.loads(r.decode())[-n_lag:]\n",
    "    df = pd.DataFrame(d)\n",
    "    original_columns=[u'close', u'date', u'high', u'low', u'open',u'volume']\n",
    "    new_columns = ['Close','Timestamp','High','Low','Open','Volume']\n",
    "    df = df.loc[:,original_columns]\n",
    "    df.columns = new_columns\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(when=None):\n",
    "    rt_df = dl_X(when)\n",
    "    rt_values = rt_df.loc[:,columns].values\n",
    "    rt_scaled = scaler.transform(rt_values)\n",
    "    rt_x = rt_scaled.reshape((1, n_lag, n_features))\n",
    "    print(rt_x.shape)\n",
    "    return rt_scaled, model.predict(rt_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do some now & past predictions\n",
    "for t in [0, 100, 200, 300, 500, 1000, 2000]:\n",
    "    rt_x, prediction = predict(time.time()-t*period)\n",
    "\n",
    "    current = rt_x[:,0]\n",
    "    prediction = prediction[0]\n",
    "\n",
    "    pyplot.plot(current, label='current')\n",
    "\n",
    "    # shift train predictions for plotting\n",
    "    predictPlot = np.empty_like(current)\n",
    "    predictPlot[:] = np.nan\n",
    "    predictPlot = np.append(predictPlot, prediction)\n",
    "\n",
    "    pyplot.plot(predictPlot, label='prediction')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction on live data!\n",
    "starttime=time.time()\n",
    "while True:\n",
    "    now = time.time() \n",
    "    end = now - now % period\n",
    "    print(time.strftime(\"%a, %d %b %Y %H:%M:%S +0000\", time.gmtime(end)))\n",
    "    rt_x, prediction = predict()\n",
    "\n",
    "    current = rt_x[:,0]\n",
    "    prediction = prediction[0]\n",
    "\n",
    "    pyplot.plot(current, label='current')\n",
    "\n",
    "    # shift train predictions for plotting\n",
    "    predictPlot = np.empty_like(current)\n",
    "    predictPlot[:] = np.nan\n",
    "    predictPlot = np.append(predictPlot, prediction)\n",
    "\n",
    "    pyplot.plot(predictPlot, label='prediction')\n",
    "    pyplot.legend()\n",
    "    pyplot.show()\n",
    "\n",
    "    time.sleep(period - ((time.time() - starttime) % period))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
